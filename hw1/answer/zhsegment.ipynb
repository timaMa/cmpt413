{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# zhsegment: default program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zhsegment import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the default solution on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中 美 在 沪 签 订 高 科 技 合 作 协 议\n",
      "新 华 社 上 海 八 月 三 十 一 日 电 （ 记 者 白 国 良 、 夏 儒 阁 ）\n",
      "“ 中 美 合 作 高 科 技 项 目 签 字 仪 式 ” 今 天 在 上 海 举 行 。\n"
     ]
    }
   ],
   "source": [
    "Pw = Pdist(data=datafile(\"../data/count_1w.txt\"))\n",
    "segmenter = Segment(Pw) # note that the default solution for this homework ignores the unigram counts\n",
    "output_full = []\n",
    "with open(\"../data/input/dev.txt\") as f:\n",
    "    for line in f:\n",
    "        output = \" \".join(segmenter.segment(line.strip()))\n",
    "        output_full.append(output)\n",
    "print(\"\\n\".join(output_full[:3])) # print out the first three lines of output as a sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the default output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score: 0.27\n"
     ]
    }
   ],
   "source": [
    "from zhsegment_check import fscore\n",
    "with open('../data/reference/dev.out', 'r') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "    tally = fscore(ref_data, output_full)\n",
    "    print(\"score: {:.2f}\".format(tally), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segment:\n",
    "\n",
    "    def __init__(self, Pw):\n",
    "        self.Pw = Pw\n",
    "\n",
    "    \n",
    "    def segment(self, text):\n",
    "        \"Return a list of words that is the best segmentation of text.\"\n",
    "        if not text: return []\n",
    "        candidates = ([first]+self.segment(rem) for first,rem in self.splits(text))\n",
    "        return max(candidates, key=self.Pwords)\n",
    "\n",
    "    def splits(self, text, L=20):\n",
    "        \"Return a list of all possible (first, rem) pairs, len(first)<=L.\"\n",
    "        return [(text[:i+1], text[i+1:]) \n",
    "                for i in range(min(len(text), L))]\n",
    "\n",
    "    def Pwords(self, words): \n",
    "        \"The Naive Bayes probability of a sequence of words.\"\n",
    "        return product(self.Pw(w) for w in words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中美在沪签订高科技合作协议\n",
      "\n",
      "('中美', '在沪签订高科技合作协议\\n')\n"
     ]
    }
   ],
   "source": [
    "Pw = Pdist(data=datafile(\"../data/count_1w.txt\"))\n",
    "segmenter = Segment(Pw) # note that the default solution for this homework ignores the unigram counts\n",
    "output_full = []\n",
    "output_orig = []\n",
    "with open(\"../data/input/dev.txt\") as f:\n",
    "    for line in f:\n",
    "        output_orig.append(line)\n",
    "        output = min(segmenter.splits(line), key=segmenter.Pwords)\n",
    "        output_full.append(output)\n",
    "print(output_orig[0])\n",
    "print(output_full[0]) # print out the first three lines of output as a sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.955908783605565e-57"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segmenter.Pwords(output_orig[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "Write some beautiful documentation of your program here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement class Entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the convenience to insert ENTRIES into the heap, we create a structure name \"Entry\", including word, starting point, log-probability of the word, backpointer for getting the best segmentation recursively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this structure, we override the functions for comparing two Entries, but this is a little different from the normal ones. \n",
    "\n",
    "For the 'equal' operation, we also implement a condition that two compared objects are not the same type, since the entry including the first word does not have back-pointer and we need to compare entry with None to figure out whether the program has achieved the first word.\n",
    "\n",
    "For the 'not equal' operation, we reverse the comparison operators, since the heap provided by Python is min-heap but we want the entry with higher probability to be at the top of the heap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Entry:\n",
    "    def __init__(self, word, startpt, logP, backpointer):\n",
    "        self.word = word\n",
    "        self.startpt = startpt\n",
    "        self.logP = logP\n",
    "        self.backpointer = backpointer\n",
    "\n",
    "    def __eq__(self, entry):\n",
    "        if not isinstance(entry, type(self)): return False\n",
    "        return (self.word == entry.word) and (self.startpt == entry.startpt) and (self.logP == entry.logP)\n",
    "\n",
    "    def __gt__(self, entry):\n",
    "        return self.logP <= entry.logP\n",
    "\n",
    "    def __lt__(self, entry):\n",
    "        return self.logP > entry.logP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implemented pseudocode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the version indicated on the website, we add some other constraints to get a higher accuracy and accelerate the program.\n",
    "\n",
    "Due to the specific features of Chinese words, it is rare to have more than 4 single words in one segmentation especially with such a content in the *dev.txt*. The *dev.txt* includes contents related to news and it is hard to have a word with more than 4 letters. So we contraint the number of letters in the missing word. We only take the missing words with less than and equal 4 letters into account. This accelerates the program dramatically, from manually interruption to less than 1 second."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 1 (using only unigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version only use unigram to implement the segmenter. \n",
    "\n",
    "The accuracy for this implementation is 0.92 (this accuracy includes the modification of the probability of missing word, which will be discussed after the implementation of the pseudocode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segment:\n",
    "\n",
    "    def __init__(self, Pw):\n",
    "        self.Pw = Pw\n",
    "\n",
    "    def segment(self, text):\n",
    "        ### Initialize the heap\n",
    "        heap = []\n",
    "        for i in range(len(text)):\n",
    "            word = text[0:i+1]\n",
    "            if word in self.Pw or len(word) <= 4:\n",
    "                heapq.heappush(heap, Entry(word, 0, log10(self.Pw(word)), None))\n",
    "                \n",
    "        ### Iteratively fill in chart[i] for all i\n",
    "        chart = {}\n",
    "        for i in range(len(text)):\n",
    "            chart[i] = Entry(None, None, None, None)\n",
    "        \n",
    "        while len(heap) > 0:\n",
    "            entry = heapq.heappop(heap)\n",
    "            endindex = entry.startpt + len(entry.word) - 1\n",
    "            if chart[endindex].backpointer is not None:\n",
    "                preventry = chart[endindex].backpointer\n",
    "                if entry.logP > preventry.logP:\n",
    "                    chart[endindex] = entry\n",
    "                if entry.logP <= preventry.logP:\n",
    "                    continue\n",
    "            else:\n",
    "                chart[endindex] = entry\n",
    "                \n",
    "            for i in range(endindex + 1, len(text)):\n",
    "                word = text[endindex + 1 : i + 1]\n",
    "                if word in self.Pw or len(word) <= 4:\n",
    "                    newentry = Entry(word, endindex + 1, entry.logP + log10(self.Pw(word)), entry)\n",
    "                    if newentry not in heap:\n",
    "                        heapq.heappush(heap, newentry)\n",
    "            \n",
    "        ### Get the best segmentation\n",
    "        finalindex = len(chart)\n",
    "        finalentry = chart[finalindex - 1]\n",
    "        segmentation = []\n",
    "        while finalentry != None:\n",
    "            segmentation.insert(0, finalentry.word)\n",
    "            finalentry = finalentry.backpointer\n",
    "\n",
    "        return segmentation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 2 (combined with bigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combine unigram model and bigram model to score word segmentation candidates. We only use bigram model for those words that both words are in the vocabulary since we can continue to use the penalty for long words that we used in the first version and we do not need to do smoothing for bigram model.\n",
    "\n",
    "We expected a higher accuracy from this model. Unfortunately, we can not make a progress on the accuracy. The accuracy of this model is still 0.92. We will analysis the possbile reason for this situation in the Analysis part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segment:\n",
    "\n",
    "    def __init__(self, uniPw, biPw):\n",
    "        self.uniPw = uniPw\n",
    "        self.biPw = biPw\n",
    "\n",
    "    def segment(self, text):\n",
    "        ### Initialize the heap\n",
    "        heap = []\n",
    "        for i in range(len(text)):\n",
    "            word = text[0:i+1]\n",
    "            if word in self.uniPw or len(word) <= 4:\n",
    "                heapq.heappush(heap, Entry(word, 0, log10(self.uniPw(word)), None))\n",
    "\n",
    "        ### Iteratively fill in chart[i] for all i\n",
    "        chart = {}\n",
    "        for i in range(len(text)):\n",
    "            chart[i] = Entry(None, None, None, None)\n",
    "\n",
    "        while len(heap) > 0:\n",
    "            entry = heapq.heappop(heap)\n",
    "            endindex = entry.startpt + len(entry.word) - 1\n",
    "            if chart[endindex].backpointer is not None:\n",
    "                preventry = chart[endindex].backpointer\n",
    "                if entry.logP > preventry.logP:\n",
    "                    chart[endindex] = entry\n",
    "                if entry.logP <= preventry.logP:\n",
    "                    continue\n",
    "            else:\n",
    "                chart[endindex] = entry\n",
    "\n",
    "            for i in range(endindex + 1, len(text)):\n",
    "                word = text[endindex + 1 : i + 1]\n",
    "                if word in self.uniPw or len(word) <= 4:\n",
    "                    wordPair = entry.word + \" \" + word\n",
    "                    uniP = self.uniPw(word)\n",
    "\n",
    "                    if wordPair in self.biPw and entry.word in self.uniPw:\n",
    "                        biP = self.biPw(wordPair) / self.uniPw(word)\n",
    "                        newentry = Entry(word, endindex + 1, entry.logP + log10(biP), entry)\n",
    "                    else:\n",
    "                        newentry = Entry(word, endindex + 1, entry.logP + log10(uniP), entry)\n",
    "                    if newentry not in heap:\n",
    "                        heapq.heappush(heap, newentry)\n",
    "\n",
    "        ### Get the best segmentation\n",
    "        finalindex = len(chart)\n",
    "        finalentry = chart[finalindex - 1]\n",
    "        segmentation = []\n",
    "        while finalentry != None:\n",
    "            segmentation.insert(0, finalentry.word)\n",
    "        finalentry = finalentry.backpointer\n",
    "\n",
    "        return segmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify probability for missing words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the features of Chinese words, the probability of long words decreases dramatically as the number of words increases. As a result, we use \n",
    "\n",
    "```1./(Total number of words * 9000 ** the length of the word*```\n",
    "\n",
    "as the probability of the missing words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pdist(dict):\n",
    "    \"A probability distribution estimated from counts in datafile.\"\n",
    "    def __init__(self, data=[], N=None, missingfn=None):\n",
    "        for key,count in data:\n",
    "            self[key] = self.get(key, 0) + int(count)\n",
    "        self.N = float(N or sum(self.values()))\n",
    "        self.missingfn = missingfn or (lambda k, N: 1./(N * 9000 ** len(k)))\n",
    "    def __call__(self, key): \n",
    "        if key in self: return self[key]/self.N  \n",
    "        else: return self.missingfn(key, self.N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run check.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中 美 在 沪 签订 高 科技 合作 协议\n",
      "新华社 上海 八月 三十一日 电 （ 记者 白 国 良 、 夏儒阁 ）\n",
      "“ 中 美 合作 高 科技 项目 签字 仪式 ” 今天 在 上海 举行 。\n"
     ]
    }
   ],
   "source": [
    "uniPw = Pdist(data=datafile(\"../data/count_1w.txt\"))\n",
    "biPw = Pdist(data=datafile(\"../data/count_1w.txt\")) \n",
    "segmenter = Segment(uniPw, biPw)\n",
    "output_full = []\n",
    "with open(\"../data/input/dev.txt\") as f:\n",
    "    for line in f:\n",
    "        output = \" \".join(segmenter.segment(line.strip()))\n",
    "        output_full.append(output)\n",
    "print(\"\\n\".join(output_full[:3])) # print out the first three lines of output as a sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score: 0.92\n"
     ]
    }
   ],
   "source": [
    "from zhsegment_check import fscore\n",
    "with open('../data/reference/dev.out', 'r') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "    tally = fscore(ref_data, output_full)\n",
    "    print(\"score: {:.2f}\".format(tally), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Do some analysis of the results. What ideas did you try? What worked and what did not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the reason why the improvement of model can not improve the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to our implementation of the model, we want to use bigram model for those words that are already in the vocabulary. We expected some improvement of the accuracy, but the result dispointed us. \n",
    "\n",
    "But after checking the *output/dev.out* and *reference/dev.out*, we found that the dispointing result is reasonable and comprehensible. We found that most words that have been predicted wrong are names. \n",
    "\n",
    "For exmaple, *白国良* has been segmented into *白 国 良* and *朱迪·梅罗* has been segmented into *朱 迪·梅罗*.\n",
    "\n",
    "For the both cases, bigram model can not help improve the accuracy since those names do not appear neither in the vocabulary file for the unigram nor in the vocabulary file for the bigram. And the reason why *朱* has been seperated is that it appears in the vocabulary list for the unigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
