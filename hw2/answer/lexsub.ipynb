{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lexsub: default program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from default import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the default solution on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n"
     ]
    }
   ],
   "source": [
    "lexsub = LexSub(os.path.join('data','glove.6B.100d.magnitude'))\n",
    "output = []\n",
    "with open(os.path.join('data','input','dev.txt')) as f:\n",
    "    for line in f:\n",
    "        fields = line.strip().split('\\t')\n",
    "        output.append(\" \".join(lexsub.substitutes(int(fields[0].strip()), fields[1].strip().split())))\n",
    "print(\"\\n\".join(output[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the default output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score=27.89\n"
     ]
    }
   ],
   "source": [
    "from lexsub_check import precision\n",
    "with open(os.path.join('data','reference','dev.out'), 'rt') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "print(\"Score={:.2f}\".format(100*precision(ref_data, output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "Write some beautiful documentation of your program here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some explanation about our code\n",
    "\n",
    "We implement a script ***retrofit.py*** to do the retrofitting. The script can detect whether the retrofitted word vectors exist. And the ***retrofit.py*** is called in ***lexsub.py*** to get the latest word vectors and lexicon.\n",
    "\n",
    "***52.97*** is the best score we can get. We have tried different methods to get better results, but we fail to do that. We will discuss the results we have tested and the reason why we fail to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lexsub import *\n",
    "import os\n",
    "from lexsub_check import precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrofit implement\n",
    "\n",
    "We implement the retrofit according to the code and the concept provided in the websites. And we have tested some different combination of T, alpha and beta. We will discuss this in the Analysis part.\n",
    "\n",
    "And we implement retrofitting to combine the information about word senses from ***ppdb-xl.txt*** in order to modify the default word vectors. After some tests, we find that the retrofitting with ***ppdb-xl.txt*** has the best result.\n",
    "\n",
    "In this retrofitting, we use iteration = 20, alpha = 2.0, beta = 1.0.\n",
    "We do not test more since the result does not have significant improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrofit(wvecs, lexicon, T = 20, alpha = 2.0, beta = 1.0):\n",
    "    vocab = set()\n",
    "    newVecs = {}\n",
    "    for word, vec in wvecs:\n",
    "        vocab.add(word)\n",
    "        newVecs[word] = vec\n",
    "\n",
    "    for i in range(T):\n",
    "        for index, word in enumerate(vocab):\n",
    "            tmpVec = np.zeros(newVecs[word].shape)\n",
    "            if word in lexicon:\n",
    "                count = 0\n",
    "                neighbors = lexicon[word]\n",
    "                for w in neighbors:\n",
    "                    if w in newVecs:\n",
    "                        tmpVec += beta * newVecs[w]\n",
    "                        count += 1\n",
    "\n",
    "                newVecs[word] = ((tmpVec + (alpha * wvecs.query(word)))) / (count + alpha)\n",
    "\n",
    "    return newVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some output after retrofitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sides bottom edge part hand place under close near along\n",
      "sides bottom edge part hand place under close near along\n",
      "sides bottom edge part hand place under close near along\n",
      "sides bottom edge part hand place under close near along\n",
      "sides bottom edge part hand place under close near along\n",
      "sides bottom edge part hand place under close near along\n",
      "sides bottom edge part hand place under close near along\n",
      "sides bottom edge part hand place under close near along\n",
      "sides bottom edge part hand place under close near along\n",
      "sides bottom edge part hand place under close near along\n"
     ]
    }
   ],
   "source": [
    "lexsub = LexSub(os.path.join(os.path.dirname(os.getcwd()), 'data','glove.6B.100d.retrofit.magnitude'))\n",
    "output = []\n",
    "with open(os.path.join(os.path.dirname(os.getcwd()), 'data','input','dev.txt')) as f:\n",
    "    for line in f:\n",
    "        fields = line.strip().split('\\t')\n",
    "        output.append(\" \".join(lexsub.substitutes(int(fields[0].strip()), fields[1].strip().split())))\n",
    "print(\"\\n\".join(output[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score=52.97\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(os.path.dirname(os.getcwd()), 'data','reference','dev.out'), 'rt') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "print(\"Score={:.2f}\".format(100*precision(ref_data, output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Do some analysis of the results. What ideas did you try? What worked and what did not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanations about Analysis part\n",
    "\n",
    "We have done two different methods tried to improve the performance.\n",
    "\n",
    "1. Adjust iterations, alpha and beta\n",
    "2. Incorporating context words\n",
    "\n",
    "The first makes little improvement to the result. The second even makes the result worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Adjust iterations, alpha and beta\n",
    "\n",
    "The best score we get is ***52.97*** with T = 20, alpha = 2.0 and beta = 1.0. The orginal selections are T = 10, alpha = 1.0 and beta = 1.0 and the result is ***52.91*** as indicated below. We improve the number of iterations hoping that the word vectors can learn better about the lexicon. However, the score only increases by 0.06. \n",
    "\n",
    "We think the reason why there is only little improvement is that the pre-trained word vectors have been trained well, and more iterations or adjusting other parameters like alpha can not make too many difference to the original word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sides edge bottom part hand place close tip under below\n",
      "sides edge bottom part hand place close tip under below\n",
      "sides edge bottom part hand place close tip under below\n",
      "sides edge bottom part hand place close tip under below\n",
      "sides edge bottom part hand place close tip under below\n",
      "sides edge bottom part hand place close tip under below\n",
      "sides edge bottom part hand place close tip under below\n",
      "sides edge bottom part hand place close tip under below\n",
      "sides edge bottom part hand place close tip under below\n",
      "sides edge bottom part hand place close tip under below\n"
     ]
    }
   ],
   "source": [
    "lexsub = LexSub(os.path.join(os.path.dirname(os.getcwd()), 'data','glove.6B.100d.retrofit.magnitude'))\n",
    "output = []\n",
    "with open(os.path.join(os.path.dirname(os.getcwd()), 'data','input','dev.txt')) as f:\n",
    "    for line in f:\n",
    "        fields = line.strip().split('\\t')\n",
    "        output.append(\" \".join(lexsub.substitutes(int(fields[0].strip()), fields[1].strip().split())))\n",
    "print(\"\\n\".join(output[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score=52.91\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(os.path.dirname(os.getcwd()), 'data','reference','dev.out'), 'rt') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "print(\"Score={:.2f}\".format(100*precision(ref_data, output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Incorporating context words\n",
    "\n",
    "We use the ***Add*** measurement decribed in the second paper in the website to calculate the arithmetic mean of the ***cos***. The equation to calculate the mean is indicated below, where C denotes the context words we use, s denotes candidate substitute words, c denotes one of the context word, t denotes the target word.\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{cos(s, t) + \\sum\\nolimits_{c \\in C} cos(s, c)} {|C|+1}\n",
    "\\end{equation}\n",
    "\n",
    "Below is the code we implement for this equation. We limit the number of candidate words to 50 since there are 400000 words in the Ontology. If every word is regarded as a candidate word, then some are useless since it is impossible for them to be substitutes and it will take too long to run the program.\n",
    "\n",
    "However, the result is not good even worse. The score is only ***25.54*** which is even worse than the default solution. After checking the result we produce and the correct answer (at the last of this notebook), we try to make some explanations about this situation. \n",
    "\n",
    "The limitation we set to the number of candidate words does not even treat ***team*** as the possible solution to the substitute of ***side***. Since the cosine similarity is only about 0.5 which is not in the top 50 most similar word in the word vectors. As a result, it is impossible to see ***team*** anywhere in the result.\n",
    "\n",
    "After figure this out, we try to get rid of the limitation. However, the running time is so long that we can not get a proper result in a short time because the algorithm has to sort words according to their means for every sentence to get the top 10 guesses.\n",
    "\n",
    "But we find that the incorporation of context words can affect the result of the 10 guesses and we think if we can run the program using 400000 words as the candidate words, we can get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substitutes(self, index, sentence):\n",
    "    \"Return ten guesses that are appropriate lexical substitutions for the word at sentence[index].\"\n",
    "    # return (list(map(lambda k: k[0], self.wvecs.most_similar(sentence[index], topn=self.topn))))\n",
    "\n",
    "    context = []\n",
    "    for i in range(5):\n",
    "        if (i is not 2) and (index - 2 + i >= 0) and (index - 2 + i <= len(sentence) - 1):\n",
    "            context.append(sentence[index - 2 + i])\n",
    "\n",
    "    candidate_sub = self.wvecs.most_similar(sentence[index], topn = 50)\n",
    "    words = []\n",
    "    nums = []\n",
    "    for (word, num) in candidate_sub:\n",
    "        words.append(word)\n",
    "        nums.append(num)\n",
    "    nums = np.array(nums)\n",
    "    for w in context:\n",
    "        tmp = np.array(self.wvecs.similarity(w, words))\n",
    "        nums = np.add(nums, tmp)\n",
    "\n",
    "    ind = np.argpartition(nums, -10)[-10:]\n",
    "    print(nums[ind])\n",
    "    return np.array(words)[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "behind on into through out further both under over during\n",
      "during part out way both under further over through on\n",
      "over further hand point during part on both through under\n",
      "around into towards out during over through both under on\n",
      "into across place under during towards part on both through\n",
      "across into hand on during through out over both under\n",
      "point further place over during part through on under both\n",
      "towards between over on during into under through across both\n",
      "towards between over on during into under through across both\n",
      "around on across both during through under between into towards\n"
     ]
    }
   ],
   "source": [
    "lexsub = LexSub(os.path.join(os.path.dirname(os.getcwd()), 'data','glove.6B.100d.retrofit.magnitude'))\n",
    "output = []\n",
    "with open(os.path.join(os.path.dirname(os.getcwd()), 'data','input','dev.txt')) as f:\n",
    "    for line in f:\n",
    "        fields = line.strip().split('\\t')\n",
    "        output.append(\" \".join(lexsub.substitutes(int(fields[0].strip()), fields[1].strip().split())))\n",
    "print(\"\\n\".join(output[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score=25.54\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(os.path.dirname(os.getcwd()), 'data','reference','dev.out'), 'rt') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "print(\"Score={:.2f}\".format(100*precision(ref_data, output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output of the candidate words for the target word in the first sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sides edge bottom part hand place close tip under below'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The substitutes provided by the human annotators for the given target word in the first sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'side.n team'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ref_data[0].replace('\\t', ' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
